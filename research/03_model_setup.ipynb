{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af6d9a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48d9b77e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\office\\\\vc\\\\research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2070bd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainingConfig:\n",
    "    root_dir: Path\n",
    "    output_dir: Path\n",
    "    phoneme_cache_path: Path\n",
    "    dataset_name: str\n",
    "    dataset_path: Path\n",
    "    metadata_path: Path\n",
    "    restore_path: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f56edd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cloner.constants import *\n",
    "from cloner.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1d5f911",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "            self,\n",
    "            config_filepath= CONFIG_FILE_PATH,\n",
    "            params_filepath= PARAMS_FILE_PATH):\n",
    "            \n",
    "            self.config=read_yaml(config_filepath)\n",
    "            self.params=read_yaml(params_filepath)\n",
    "\n",
    "            create_directories([self.config.artifacts_root]) \n",
    "\n",
    "    def get_model_training_config(self)-> ModelTrainingConfig:\n",
    "          config=self.config.model_training\n",
    "          create_directories([config.root_dir]) \n",
    "\n",
    "          model_training_config=ModelTrainingConfig( \n",
    "                root_dir= config.root_dir,\n",
    "                output_dir= config.output_dir,\n",
    "                phoneme_cache_path= config.phoneme_cache_path,\n",
    "                dataset_name= config.dataset_name,\n",
    "                dataset_path= config.dataset_path,\n",
    "                metadata_path= config.metadata_path,\n",
    "                restore_path= config.restore_path\n",
    "                ) \n",
    "\n",
    "          return model_training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba0ccc77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-05-22 16:06:05,506: DEBUG: __init__: pydot initializing]\n",
      "[2025-05-22 16:06:05,508: DEBUG: __init__: pydot 4.0.0]\n",
      "[2025-05-22 16:06:05,524: DEBUG: core: pydot core module initializing]\n",
      "[2025-05-22 16:06:11,311: DEBUG: utils: Loading FFmpeg6]\n",
      "[2025-05-22 16:06:11,332: DEBUG: utils: Failed to load FFmpeg6 extension.]\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\miniconda\\envs\\vc\\lib\\site-packages\\torio\\_extension\\utils.py\", line 116, in _find_ffmpeg_extension\n",
      "    ext = _find_versionsed_ffmpeg_extension(ffmpeg_ver)\n",
      "  File \"d:\\miniconda\\envs\\vc\\lib\\site-packages\\torio\\_extension\\utils.py\", line 108, in _find_versionsed_ffmpeg_extension\n",
      "    _load_lib(lib)\n",
      "  File \"d:\\miniconda\\envs\\vc\\lib\\site-packages\\torio\\_extension\\utils.py\", line 94, in _load_lib\n",
      "    torch.ops.load_library(path)\n",
      "  File \"d:\\miniconda\\envs\\vc\\lib\\site-packages\\torch\\_ops.py\", line 1350, in load_library\n",
      "    ctypes.CDLL(path)\n",
      "  File \"d:\\miniconda\\envs\\vc\\lib\\ctypes\\__init__.py\", line 374, in __init__\n",
      "    self._handle = _dlopen(self._name, mode)\n",
      "FileNotFoundError: Could not find module 'D:\\miniconda\\envs\\vc\\Lib\\site-packages\\torio\\lib\\libtorio_ffmpeg6.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "[2025-05-22 16:06:11,377: DEBUG: utils: Loading FFmpeg5]\n",
      "[2025-05-22 16:06:11,394: DEBUG: utils: Failed to load FFmpeg5 extension.]\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\miniconda\\envs\\vc\\lib\\site-packages\\torio\\_extension\\utils.py\", line 116, in _find_ffmpeg_extension\n",
      "    ext = _find_versionsed_ffmpeg_extension(ffmpeg_ver)\n",
      "  File \"d:\\miniconda\\envs\\vc\\lib\\site-packages\\torio\\_extension\\utils.py\", line 108, in _find_versionsed_ffmpeg_extension\n",
      "    _load_lib(lib)\n",
      "  File \"d:\\miniconda\\envs\\vc\\lib\\site-packages\\torio\\_extension\\utils.py\", line 94, in _load_lib\n",
      "    torch.ops.load_library(path)\n",
      "  File \"d:\\miniconda\\envs\\vc\\lib\\site-packages\\torch\\_ops.py\", line 1350, in load_library\n",
      "    ctypes.CDLL(path)\n",
      "  File \"d:\\miniconda\\envs\\vc\\lib\\ctypes\\__init__.py\", line 374, in __init__\n",
      "    self._handle = _dlopen(self._name, mode)\n",
      "FileNotFoundError: Could not find module 'D:\\miniconda\\envs\\vc\\Lib\\site-packages\\torio\\lib\\libtorio_ffmpeg5.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "[2025-05-22 16:06:11,397: DEBUG: utils: Loading FFmpeg4]\n",
      "[2025-05-22 16:06:11,415: DEBUG: utils: Failed to load FFmpeg4 extension.]\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\miniconda\\envs\\vc\\lib\\site-packages\\torio\\_extension\\utils.py\", line 116, in _find_ffmpeg_extension\n",
      "    ext = _find_versionsed_ffmpeg_extension(ffmpeg_ver)\n",
      "  File \"d:\\miniconda\\envs\\vc\\lib\\site-packages\\torio\\_extension\\utils.py\", line 108, in _find_versionsed_ffmpeg_extension\n",
      "    _load_lib(lib)\n",
      "  File \"d:\\miniconda\\envs\\vc\\lib\\site-packages\\torio\\_extension\\utils.py\", line 94, in _load_lib\n",
      "    torch.ops.load_library(path)\n",
      "  File \"d:\\miniconda\\envs\\vc\\lib\\site-packages\\torch\\_ops.py\", line 1350, in load_library\n",
      "    ctypes.CDLL(path)\n",
      "  File \"d:\\miniconda\\envs\\vc\\lib\\ctypes\\__init__.py\", line 374, in __init__\n",
      "    self._handle = _dlopen(self._name, mode)\n",
      "FileNotFoundError: Could not find module 'D:\\miniconda\\envs\\vc\\Lib\\site-packages\\torio\\lib\\libtorio_ffmpeg4.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "[2025-05-22 16:06:11,420: DEBUG: utils: Loading FFmpeg]\n",
      "[2025-05-22 16:06:11,422: DEBUG: utils: Failed to load FFmpeg extension.]\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\miniconda\\envs\\vc\\lib\\site-packages\\torio\\_extension\\utils.py\", line 116, in _find_ffmpeg_extension\n",
      "    ext = _find_versionsed_ffmpeg_extension(ffmpeg_ver)\n",
      "  File \"d:\\miniconda\\envs\\vc\\lib\\site-packages\\torio\\_extension\\utils.py\", line 106, in _find_versionsed_ffmpeg_extension\n",
      "    raise RuntimeError(f\"FFmpeg{version} extension is not available.\")\n",
      "RuntimeError: FFmpeg extension is not available.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from trainer import Trainer, TrainerArgs\n",
    "from TTS.tts.configs.vits_config import VitsConfig\n",
    "from TTS.tts.configs.shared_configs import BaseDatasetConfig\n",
    "from TTS.tts.datasets import load_tts_samples\n",
    "from TTS.tts.models.vits import Vits\n",
    "from TTS.tts.utils.text.tokenizer import TTSTokenizer\n",
    "from cloner.utils.common import read_yaml\n",
    "from cloner.pipeline.stage_02_data_preprocessing import DataPreprocessor\n",
    "from cloner.entity.config_entity import DataPreProcessConfig\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57045a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConfig:\n",
    "    def __init__(self,config: ModelTrainingConfig):\n",
    "        self.config=config\n",
    "        self.params=read_yaml(PARAMS_FILE_PATH)\n",
    "\n",
    "        self.audio_config=self.get_audio_config()\n",
    "        self.dataset_config=self.get_dataset_config()\n",
    "        self.vits_config=self.get_vits_config()\n",
    "\n",
    "        self._audio_processor=None\n",
    "        self._tokenizer=None\n",
    "        self._model=None\n",
    "        self._trainer_instance=None\n",
    "        self._train_samples=None\n",
    "        self._eval_samples=None\n",
    "\n",
    "    def get_audio_config(self):\n",
    "        return self.params[\"audio_config\"]\n",
    "\n",
    "    def get_dataset_config(self):\n",
    "        return BaseDatasetConfig(\n",
    "            formatter=self.config.dataset_name,\n",
    "            meta_file_train=self.config.metadata_path,\n",
    "            path=self.config.dataset_path\n",
    "        )\n",
    "\n",
    "    def get_vits_config(self):\n",
    "        config=self.config\n",
    "        params=self.params[\"model_config\"]\n",
    "        audio_config=self.audio_config\n",
    "        dataset_config=self.dataset_config\n",
    "        return VitsConfig(\n",
    "            audio=audio_config,\n",
    "            run_name=params[\"run_name\"],\n",
    "            batch_size=params[\"batch_size\"],\n",
    "            eval_batch_size=params[\"eval_batch_size\"],\n",
    "            batch_group_size=params[\"batch_group_size\"],\n",
    "            num_loader_workers=params[\"num_loader_workers\"],\n",
    "            num_eval_loader_workers=params[\"num_eval_loader_workers\"],\n",
    "            run_eval=params[\"run_eval\"],\n",
    "            test_delay_epochs=params[\"test_delay_epochs\"],\n",
    "            epochs=params[\"epochs\"],\n",
    "            text_cleaner=params[\"text_cleaner\"],\n",
    "            use_phonemes=params[\"use_phonemes\"],\n",
    "            phoneme_language=params[\"phoneme_language\"],\n",
    "            phoneme_cache_path=os.path.join(params[\"output_path\"], \"phoneme_cache\"),\n",
    "            compute_input_seq_cache=params[\"compute_input_seq_cache\"],\n",
    "            print_step=params[\"print_step\"],\n",
    "            print_eval=params[\"print_eval\"],\n",
    "            mixed_precision=params[\"mixed_precision\"],\n",
    "            output_path=params[\"output_path\"],\n",
    "            datasets=[dataset_config],\n",
    "            cudnn_benchmark=params[\"cudnn_benchmark\"],\n",
    "        )\n",
    "\n",
    "    def get_audio_processor(self):\n",
    "        if self._audio_processor is None:\n",
    "            data_preprocess_config=DataPreProcessConfig(\n",
    "                root_dir=self.config.root_dir,\n",
    "                processed_audio_dir=\"\",  \n",
    "                audio_path=\"\"         \n",
    "            )\n",
    "            processor=DataPreprocessor(config=data_preprocess_config)\n",
    "            self._audio_processor=processor.get_audio_processor()\n",
    "        return self._audio_processor\n",
    "\n",
    "    def get_tokenizer(self):\n",
    "            if self._tokenizer is None:\n",
    "                vits_config=self.vits_config\n",
    "                tokenizer, config=TTSTokenizer.init_from_config(vits_config)\n",
    "                self._tokenizer=tokenizer\n",
    "            return self._tokenizer\n",
    "    \n",
    "    def get_data_split(self):\n",
    "        if self._train_samples is None or self._eval_samples is None: \n",
    "            self._train_samples,self._eval_samples=load_tts_samples(\n",
    "                self.dataset_config,\n",
    "                eval_split=True,\n",
    "                eval_split_max_size=self.vits_config.eval_split_max_size,\n",
    "                eval_split_size=self.vits_config.eval_split_size,\n",
    "            )\n",
    "        return self._train_samples,self._eval_samples\n",
    "    \n",
    "    def get_model(self, checkpoint_path=None):\n",
    "        if self._model is None:\n",
    "            config=self.vits_config\n",
    "            ap=self.get_audio_processor()\n",
    "            tokenizer=self.get_tokenizer()\n",
    "            self._model=Vits(config,ap,tokenizer,speaker_manager=None)\n",
    "            return self._model\n",
    "        if checkpoint_path:\n",
    "            checkpoint=torch.load(checkpoint_path)\n",
    "            self._model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            epoch=checkpoint.get('epoch',0)\n",
    "            print(f\"Resuming from checkpoint at epoch{epoch}\")\n",
    "        return self._model\n",
    "    \n",
    "    def get_trainer(self, restore_path=None):\n",
    "        train_samples, eval_samples = self.get_data_split()\n",
    "\n",
    "        model = self.get_model()\n",
    "        \n",
    "        trainer_args = TrainerArgs()\n",
    "        trainer_args.restore_path = restore_path \n",
    "        trainer_instance = Trainer(\n",
    "            trainer_args,\n",
    "            config=self.vits_config,\n",
    "            output_path=self.config.output_dir,\n",
    "            model=model,\n",
    "            train_samples=train_samples,\n",
    "            eval_samples=eval_samples,\n",
    "            parse_command_line_args=False\n",
    "        )\n",
    "        return trainer_instance\n",
    "    \n",
    "    def load_model_from_checkpoint(self, restore_path): \n",
    "        if os.path.exists(restore_path):\n",
    "            checkpoint = torch.load(restore_path, map_location=\"cpu\")\n",
    "            model = self.get_model()\n",
    "            model.load_state_dict(checkpoint[\"model\"])\n",
    "            optimizer = checkpoint[\"optimizer\"]\n",
    "            epoch = checkpoint[\"epoch\"]\n",
    "            step = checkpoint[\"step\"]\n",
    "            return model, optimizer, epoch, step\n",
    "        else:\n",
    "            return None, None, 0, 0\n",
    "    def get_fit(self):\n",
    "        restore_path = getattr(self.config, \"restore_path\", None)\n",
    "        trainer = self.get_trainer(restore_path)\n",
    "        trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9020847",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dinaa\\AppData\\Local\\Temp\\ipykernel_14320\\1812373493.py:118: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(restore_path, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Setting up Audio Processor...\n",
      " | > sample_rate:22050\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log10\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:12.5\n",
      " | > frame_length_ms:50\n",
      " | > ref_level_db:0\n",
      " | > fft_size:2400\n",
      " | > power:1.5\n",
      " | > preemphasis:0.98\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:True\n",
      " | > symmetric_norm:False\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:8000.0\n",
      " | > pitch_fmin:1.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:20.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:1.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:True\n",
      " | > trim_db:45\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:None\n",
      " | > base:10\n",
      " | > win_length:1102\n",
      " | > hop_length:275\n",
      " | > Found 829 files in D:\\office\\vc\\artifacts\\data_ingestion\\LJSpeech-1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " > Training Environment:\n",
      " | > Backend: Torch\n",
      " | > Mixed precision: True\n",
      " | > Precision: fp16\n",
      " | > Current device: 0\n",
      " | > Num. of GPUs: 1\n",
      " | > Num. of CPUs: 12\n",
      " | > Num. of Torch Threads: 6\n",
      " | > Torch seed: 54321\n",
      " | > Torch CUDNN: True\n",
      " | > Torch CUDNN deterministic: False\n",
      " | > Torch CUDNN benchmark: False\n",
      " | > Torch TF32 MatMul: False\n",
      " > Start Tensorboard: tensorboard --logdir=artifacts/model_training/output\\vits_ljspeech-May-22-2025_04+12PM-b0c777f\n",
      " > Restoring from best_model.pth ...\n",
      " > Restoring Model...\n",
      " > Restoring Optimizer...\n",
      " > Restoring Scaler...\n",
      " > Model restored from step 927\n",
      "\n",
      " > Model has 83059180 parameters\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 0/10\u001b[0m\n",
      " --> artifacts/model_training/output\\vits_ljspeech-May-22-2025_04+12PM-b0c777f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "> DataLoader initialization\n",
      "| > Tokenizer:\n",
      "\t| > add_blank: True\n",
      "\t| > use_eos_bos: False\n",
      "\t| > use_phonemes: True\n",
      "\t| > phonemizer:\n",
      "\t\t| > phoneme language: en-us\n",
      "\t\t| > phoneme backend: gruut\n",
      "| > Number of instances : 821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m > TRAINING (2025-05-22 16:12:36) \u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | > Preprocessing samples\n",
      " | > Max text length: 173\n",
      " | > Min text length: 6\n",
      " | > Avg text length: 55.60535931790499\n",
      " | \n",
      " | > Max audio length: 358870.0\n",
      " | > Min audio length: 25558.0\n",
      " | > Avg audio length: 110688.91352009744\n",
      " | > Num. instances discarded samples: 0\n",
      " | > Batch group size: 16.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda\\envs\\vc\\lib\\site-packages\\torch\\functional.py:704: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
      "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\SpectralOps.cpp:878.)\n",
      "  return _VF.stft(  # type: ignore[attr-defined]\n",
      "d:\\miniconda\\envs\\vc\\lib\\site-packages\\TTS\\tts\\models\\vits.py:1273: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=False):  # use float32 for the criterion\n",
      "d:\\miniconda\\envs\\vc\\lib\\site-packages\\TTS\\tts\\models\\vits.py:1284: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=False):\n",
      "d:\\miniconda\\envs\\vc\\lib\\site-packages\\TTS\\tts\\models\\vits.py:1311: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=False):  # use float32 for the criterion\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-22 16:15:09 -- STEP: 22/103 -- GLOBAL_STEP: 950\u001b[0m\n",
      "     | > loss_disc: 2.0995562076568604  (2.1857545538382093)\n",
      "     | > loss_disc_real_0: 0.10675552487373352  (0.13845491646365682)\n",
      "     | > loss_disc_real_1: 0.21112851798534393  (0.23630940677090126)\n",
      "     | > loss_disc_real_2: 0.2091480791568756  (0.2078784436664798)\n",
      "     | > loss_disc_real_3: 0.19782444834709167  (0.20781989666548642)\n",
      "     | > loss_disc_real_4: 0.24726741015911102  (0.19930819896134463)\n",
      "     | > loss_disc_real_5: 0.20502349734306335  (0.22608573429963805)\n",
      "     | > loss_0: 2.0995562076568604  (2.1857545538382093)\n",
      "     | > grad_norm_0: tensor(31.6018, device='cuda:0')  (tensor(21.5966, device='cuda:0'))\n",
      "     | > loss_gen: 2.64772629737854  (2.479180617765947)\n",
      "     | > loss_kl: 1.6061460971832275  (1.6247183301232078)\n",
      "     | > loss_feat: 4.279690742492676  (3.978840957988392)\n",
      "     | > loss_mel: 38.978485107421875  (36.99626350402832)\n",
      "     | > loss_duration: 2.5379133224487305  (2.5773351192474365)\n",
      "     | > amp_scaler: 64.0  (1605.8181818181818)\n",
      "     | > loss_1: 50.049964904785156  (47.6563382582231)\n",
      "     | > grad_norm_1: tensor(1266.3165, device='cuda:0')  (tensor(289.8026, device='cuda:0'))\n",
      "     | > current_lr_0: 0.0002 \n",
      "     | > current_lr_1: 0.0002 \n",
      "     | > step_time: 1.3205  (1.3157775727185337)\n",
      "     | > loader_time: 0.0085  (0.009798981926657936)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-22 16:15:40 -- STEP: 47/103 -- GLOBAL_STEP: 975\u001b[0m\n",
      "     | > loss_disc: 2.29532790184021  (2.2744086930092338)\n",
      "     | > loss_disc_real_0: 0.09326756000518799  (0.14713175642363568)\n",
      "     | > loss_disc_real_1: 0.18944622576236725  (0.2415464235113022)\n",
      "     | > loss_disc_real_2: 0.08780906349420547  (0.2285529387758133)\n",
      "     | > loss_disc_real_3: 0.2374219000339508  (0.24416106368633025)\n",
      "     | > loss_disc_real_4: 0.36472150683403015  (0.23177533200446596)\n",
      "     | > loss_disc_real_5: 0.1762966811656952  (0.23106933955816514)\n",
      "     | > loss_0: 2.29532790184021  (2.2744086930092338)\n",
      "     | > grad_norm_0: tensor(32.1248, device='cuda:0')  (tensor(27.1263, device='cuda:0'))\n",
      "     | > loss_gen: 2.2490408420562744  (2.5704091356155723)\n",
      "     | > loss_kl: 1.2213563919067383  (1.6353903146500284)\n",
      "     | > loss_feat: 3.554872989654541  (3.9639136436137767)\n",
      "     | > loss_mel: 33.13873291015625  (39.085720711566054)\n",
      "     | > loss_duration: 2.291386127471924  (2.432060282281105)\n",
      "     | > amp_scaler: 64.0  (785.7021276595744)\n",
      "     | > loss_1: 42.45539093017578  (49.687493750389585)\n",
      "     | > grad_norm_1: tensor(441.4745, device='cuda:0')  (tensor(271.8737, device='cuda:0'))\n",
      "     | > current_lr_0: 0.0002 \n",
      "     | > current_lr_1: 0.0002 \n",
      "     | > step_time: 1.2591  (1.2823088118370538)\n",
      "     | > loader_time: 0.0086  (0.009135773841370928)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-22 16:16:22 -- STEP: 72/103 -- GLOBAL_STEP: 1000\u001b[0m\n",
      "     | > loss_disc: 2.37787127494812  (2.2770873953898745)\n",
      "     | > loss_disc_real_0: 0.11154982447624207  (0.13956039616217214)\n",
      "     | > loss_disc_real_1: 0.14317114651203156  (0.23946316374672783)\n",
      "     | > loss_disc_real_2: 0.21739059686660767  (0.23185242360664737)\n",
      "     | > loss_disc_real_3: 0.2255716323852539  (0.23431716962820953)\n",
      "     | > loss_disc_real_4: 0.22136279940605164  (0.23238293329874674)\n",
      "     | > loss_disc_real_5: 0.09879538416862488  (0.22602428547624084)\n",
      "     | > loss_0: 2.37787127494812  (2.2770873953898745)\n",
      "     | > grad_norm_0: tensor(51.8631, device='cuda:0')  (tensor(28.9452, device='cuda:0'))\n",
      "     | > loss_gen: 2.540008068084717  (2.532983548111386)\n",
      "     | > loss_kl: 1.380751371383667  (1.572291807995902)\n",
      "     | > loss_feat: 3.6172239780426025  (3.807978726095625)\n",
      "     | > loss_mel: 38.30096435546875  (38.746563487582755)\n",
      "     | > loss_duration: 2.2214107513427734  (2.361449079381096)\n",
      "     | > amp_scaler: 64.0  (535.1111111111111)\n",
      "     | > loss_1: 48.06035614013672  (49.0212664074368)\n",
      "     | > grad_norm_1: tensor(310.3946, device='cuda:0')  (tensor(285.8545, device='cuda:0'))\n",
      "     | > current_lr_0: 0.0002 \n",
      "     | > current_lr_1: 0.0002 \n",
      "     | > step_time: 4.1259  (1.4091479910744558)\n",
      "     | > loader_time: 0.0145  (0.009497801462809244)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config=ConfigurationManager()\n",
    "    model_training_config=config.get_model_training_config()\n",
    "    model_training=ModelConfig(config=model_training_config)\n",
    "    model_training.get_audio_config()\n",
    "    model_training.get_dataset_config()\n",
    "    model_training.get_vits_config()\n",
    "    model_training.get_fit()    \n",
    "    \n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0f7cdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
