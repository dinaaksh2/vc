{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af6d9a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48d9b77e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\office\\\\vc'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2070bd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainingConfig:\n",
    "    root_dir: Path\n",
    "    output_dir: Path\n",
    "    phoneme_cache_path: Path\n",
    "    dataset_name: str\n",
    "    dataset_path: Path\n",
    "    metadata_path: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f56edd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cloner.constants import *\n",
    "from cloner.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1d5f911",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "            self,\n",
    "            config_filepath= CONFIG_FILE_PATH,\n",
    "            params_filepath= PARAMS_FILE_PATH):\n",
    "            \n",
    "            self.config=read_yaml(config_filepath)\n",
    "            self.params=read_yaml(params_filepath)\n",
    "\n",
    "            create_directories([self.config.artifacts_root]) \n",
    "\n",
    "    def get_model_training_config(self)-> ModelTrainingConfig:\n",
    "          config=self.config.model_training\n",
    "          create_directories([config.root_dir]) \n",
    "\n",
    "          model_training_config=ModelTrainingConfig( \n",
    "                root_dir= config.root_dir,\n",
    "                output_dir= config.output_dir,\n",
    "                phoneme_cache_path= config.phoneme_cache_path,\n",
    "                dataset_name= config.dataset_name,\n",
    "                dataset_path= config.dataset_path,\n",
    "                metadata_path= config.metadata_path\n",
    "                ) \n",
    "\n",
    "          return model_training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba0ccc77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-05-22 13:27:31,934: DEBUG: __init__: pydot initializing]\n",
      "[2025-05-22 13:27:31,936: DEBUG: __init__: pydot 4.0.0]\n",
      "[2025-05-22 13:27:31,952: DEBUG: core: pydot core module initializing]\n",
      "[2025-05-22 13:27:37,715: DEBUG: utils: Loading FFmpeg6]\n",
      "[2025-05-22 13:27:37,735: DEBUG: utils: Failed to load FFmpeg6 extension.]\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\miniconda\\envs\\vc\\lib\\site-packages\\torio\\_extension\\utils.py\", line 116, in _find_ffmpeg_extension\n",
      "    ext = _find_versionsed_ffmpeg_extension(ffmpeg_ver)\n",
      "  File \"d:\\miniconda\\envs\\vc\\lib\\site-packages\\torio\\_extension\\utils.py\", line 108, in _find_versionsed_ffmpeg_extension\n",
      "    _load_lib(lib)\n",
      "  File \"d:\\miniconda\\envs\\vc\\lib\\site-packages\\torio\\_extension\\utils.py\", line 94, in _load_lib\n",
      "    torch.ops.load_library(path)\n",
      "  File \"d:\\miniconda\\envs\\vc\\lib\\site-packages\\torch\\_ops.py\", line 1350, in load_library\n",
      "    ctypes.CDLL(path)\n",
      "  File \"d:\\miniconda\\envs\\vc\\lib\\ctypes\\__init__.py\", line 374, in __init__\n",
      "    self._handle = _dlopen(self._name, mode)\n",
      "FileNotFoundError: Could not find module 'D:\\miniconda\\envs\\vc\\Lib\\site-packages\\torio\\lib\\libtorio_ffmpeg6.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "[2025-05-22 13:27:37,741: DEBUG: utils: Loading FFmpeg5]\n",
      "[2025-05-22 13:27:37,756: DEBUG: utils: Failed to load FFmpeg5 extension.]\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\miniconda\\envs\\vc\\lib\\site-packages\\torio\\_extension\\utils.py\", line 116, in _find_ffmpeg_extension\n",
      "    ext = _find_versionsed_ffmpeg_extension(ffmpeg_ver)\n",
      "  File \"d:\\miniconda\\envs\\vc\\lib\\site-packages\\torio\\_extension\\utils.py\", line 108, in _find_versionsed_ffmpeg_extension\n",
      "    _load_lib(lib)\n",
      "  File \"d:\\miniconda\\envs\\vc\\lib\\site-packages\\torio\\_extension\\utils.py\", line 94, in _load_lib\n",
      "    torch.ops.load_library(path)\n",
      "  File \"d:\\miniconda\\envs\\vc\\lib\\site-packages\\torch\\_ops.py\", line 1350, in load_library\n",
      "    ctypes.CDLL(path)\n",
      "  File \"d:\\miniconda\\envs\\vc\\lib\\ctypes\\__init__.py\", line 374, in __init__\n",
      "    self._handle = _dlopen(self._name, mode)\n",
      "FileNotFoundError: Could not find module 'D:\\miniconda\\envs\\vc\\Lib\\site-packages\\torio\\lib\\libtorio_ffmpeg5.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "[2025-05-22 13:27:37,759: DEBUG: utils: Loading FFmpeg4]\n",
      "[2025-05-22 13:27:37,775: DEBUG: utils: Failed to load FFmpeg4 extension.]\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\miniconda\\envs\\vc\\lib\\site-packages\\torio\\_extension\\utils.py\", line 116, in _find_ffmpeg_extension\n",
      "    ext = _find_versionsed_ffmpeg_extension(ffmpeg_ver)\n",
      "  File \"d:\\miniconda\\envs\\vc\\lib\\site-packages\\torio\\_extension\\utils.py\", line 108, in _find_versionsed_ffmpeg_extension\n",
      "    _load_lib(lib)\n",
      "  File \"d:\\miniconda\\envs\\vc\\lib\\site-packages\\torio\\_extension\\utils.py\", line 94, in _load_lib\n",
      "    torch.ops.load_library(path)\n",
      "  File \"d:\\miniconda\\envs\\vc\\lib\\site-packages\\torch\\_ops.py\", line 1350, in load_library\n",
      "    ctypes.CDLL(path)\n",
      "  File \"d:\\miniconda\\envs\\vc\\lib\\ctypes\\__init__.py\", line 374, in __init__\n",
      "    self._handle = _dlopen(self._name, mode)\n",
      "FileNotFoundError: Could not find module 'D:\\miniconda\\envs\\vc\\Lib\\site-packages\\torio\\lib\\libtorio_ffmpeg4.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "[2025-05-22 13:27:37,779: DEBUG: utils: Loading FFmpeg]\n",
      "[2025-05-22 13:27:37,781: DEBUG: utils: Failed to load FFmpeg extension.]\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\miniconda\\envs\\vc\\lib\\site-packages\\torio\\_extension\\utils.py\", line 116, in _find_ffmpeg_extension\n",
      "    ext = _find_versionsed_ffmpeg_extension(ffmpeg_ver)\n",
      "  File \"d:\\miniconda\\envs\\vc\\lib\\site-packages\\torio\\_extension\\utils.py\", line 106, in _find_versionsed_ffmpeg_extension\n",
      "    raise RuntimeError(f\"FFmpeg{version} extension is not available.\")\n",
      "RuntimeError: FFmpeg extension is not available.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from trainer import Trainer, TrainerArgs\n",
    "from TTS.tts.configs.vits_config import VitsConfig\n",
    "from TTS.tts.configs.shared_configs import BaseDatasetConfig\n",
    "from TTS.tts.datasets import load_tts_samples\n",
    "from TTS.tts.models.vits import Vits\n",
    "from TTS.tts.utils.text.tokenizer import TTSTokenizer\n",
    "from cloner.utils.common import read_yaml\n",
    "from cloner.pipeline.stage_02_data_preprocessing import DataPreprocessor\n",
    "from cloner.entity.config_entity import DataPreProcessConfig\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57045a8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "class ModelConfig:\n",
    "    def __init__(self,config: ModelTrainingConfig):\n",
    "        self.config=config\n",
    "        self.params=read_yaml(PARAMS_FILE_PATH)\n",
    "\n",
    "        self.audio_config=self.get_audio_config()\n",
    "        self.dataset_config=self.get_dataset_config()\n",
    "        self.vits_config=self.get_vits_config()\n",
    "\n",
    "        self._audio_processor=None\n",
    "        self._tokenizer=None\n",
    "        self._model=None\n",
    "        self._trainer_instance=None\n",
    "        self._train_samples=None\n",
    "        self._eval_samples=None\n",
    "\n",
    "    def get_audio_config(self):\n",
    "        return self.params[\"audio_config\"]\n",
    "\n",
    "    def get_dataset_config(self):\n",
    "        return BaseDatasetConfig(\n",
    "            formatter=self.config.dataset_name,\n",
    "            meta_file_train=self.config.metadata_path,\n",
    "            path=self.config.dataset_path\n",
    "        )\n",
    "\n",
    "    def get_vits_config(self):\n",
    "        config=self.config\n",
    "        params=self.params[\"model_config\"]\n",
    "        audio_config=self.audio_config\n",
    "        dataset_config=self.dataset_config\n",
    "        return VitsConfig(\n",
    "            audio=audio_config,\n",
    "            run_name=params[\"run_name\"],\n",
    "            batch_size=params[\"batch_size\"],\n",
    "            eval_batch_size=params[\"eval_batch_size\"],\n",
    "            batch_group_size=params[\"batch_group_size\"],\n",
    "            num_loader_workers=params[\"num_loader_workers\"],\n",
    "            num_eval_loader_workers=params[\"num_eval_loader_workers\"],\n",
    "            run_eval=params[\"run_eval\"],\n",
    "            test_delay_epochs=params[\"test_delay_epochs\"],\n",
    "            epochs=params[\"epochs\"],\n",
    "            text_cleaner=params[\"text_cleaner\"],\n",
    "            use_phonemes=params[\"use_phonemes\"],\n",
    "            phoneme_language=params[\"phoneme_language\"],\n",
    "            phoneme_cache_path=os.path.join(params[\"output_path\"], \"phoneme_cache\"),\n",
    "            compute_input_seq_cache=params[\"compute_input_seq_cache\"],\n",
    "            print_step=params[\"print_step\"],\n",
    "            print_eval=params[\"print_eval\"],\n",
    "            mixed_precision=params[\"mixed_precision\"],\n",
    "            output_path=params[\"output_path\"],\n",
    "            datasets=[dataset_config],\n",
    "            cudnn_benchmark=params[\"cudnn_benchmark\"],\n",
    "        )\n",
    "\n",
    "    def get_audio_processor(self):\n",
    "        if self._audio_processor is None:\n",
    "            data_preprocess_config=DataPreProcessConfig(\n",
    "                root_dir=self.config.root_dir,\n",
    "                processed_audio_dir=\"\",  \n",
    "                audio_path=\"\"         \n",
    "            )\n",
    "            processor=DataPreprocessor(config=data_preprocess_config)\n",
    "            self._audio_processor=processor.get_audio_processor()\n",
    "        return self._audio_processor\n",
    "\n",
    "    def get_tokenizer(self):\n",
    "            if self._tokenizer is None:\n",
    "                vits_config=self.vits_config\n",
    "                tokenizer, config=TTSTokenizer.init_from_config(vits_config)\n",
    "                self._tokenizer=tokenizer\n",
    "            return self._tokenizer\n",
    "    \n",
    "    def get_data_split(self):\n",
    "        if self._train_samples is None or self._eval_samples is None: \n",
    "            self._train_samples,self._eval_samples=load_tts_samples(\n",
    "                self.dataset_config,\n",
    "                eval_split=True,\n",
    "                eval_split_max_size=self.vits_config.eval_split_max_size,\n",
    "                eval_split_size=self.vits_config.eval_split_size,\n",
    "            )\n",
    "        return self._train_samples,self._eval_samples\n",
    "    \n",
    "    def get_model(self):\n",
    "        if self._model is None:\n",
    "            config=self.vits_config\n",
    "            ap=self.get_audio_processor()\n",
    "            tokenizer=self.get_tokenizer()\n",
    "            self._model=Vits(config,ap,tokenizer,speaker_manager=None)\n",
    "            return self._model\n",
    "    \n",
    "    def get_trainer(self):\n",
    "        if self._trainer_instance is None:\n",
    "            train_samples,eval_samples=self.get_data_split()\n",
    "            self._trainer_instance=Trainer(\n",
    "                TrainerArgs(),\n",
    "                config=self.vits_config,\n",
    "                output_path=self.config.output_dir,\n",
    "                model=self.get_model(),\n",
    "                train_samples=train_samples,\n",
    "                eval_samples=eval_samples,\n",
    "                parse_command_line_args=False\n",
    "            )\n",
    "        return self._trainer_instance\n",
    "\n",
    "    def get_fit(self):\n",
    "        trainer=self.get_trainer()\n",
    "        if trainer is None:\n",
    "            raise ValueError(\"Trainer instance is None. Cannot start training.\")\n",
    "        trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9020847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | > Found 829 files in D:\\office\\vc\\artifacts\\data_ingestion\\LJSpeech-1.1\n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:22050\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log10\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:12.5\n",
      " | > frame_length_ms:50\n",
      " | > ref_level_db:0\n",
      " | > fft_size:2400\n",
      " | > power:1.5\n",
      " | > preemphasis:0.98\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:True\n",
      " | > symmetric_norm:False\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:8000.0\n",
      " | > pitch_fmin:1.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:20.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:1.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:True\n",
      " | > trim_db:45\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:None\n",
      " | > base:10\n",
      " | > win_length:1102\n",
      " | > hop_length:275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " > Training Environment:\n",
      " | > Backend: Torch\n",
      " | > Mixed precision: False\n",
      " | > Precision: float32\n",
      " | > Current device: 0\n",
      " | > Num. of GPUs: 1\n",
      " | > Num. of CPUs: 12\n",
      " | > Num. of Torch Threads: 6\n",
      " | > Torch seed: 54321\n",
      " | > Torch CUDNN: True\n",
      " | > Torch CUDNN deterministic: False\n",
      " | > Torch CUDNN benchmark: False\n",
      " | > Torch TF32 MatMul: False\n",
      " > Start Tensorboard: tensorboard --logdir=artifacts/model_training/output\\vits_ljspeech-May-22-2025_01+27PM-f4cb597\n",
      "\n",
      " > Model has 83059180 parameters\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 0/200\u001b[0m\n",
      " --> artifacts/model_training/output\\vits_ljspeech-May-22-2025_01+27PM-f4cb597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Pre-computing phonemes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/821 [00:02<10:03,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi ɪsnt æz bɪɡ bʌt hz stɪl kwaɪt pɑpjəlɚ ɪv hɚd ðə seɪm θɪŋ əbaʊt hɪz kɑntɛnt nɛvɚ wɔt͡ʃt hɪm mʌt͡ʃ\n",
      " [!] Character '͡' not found in the vocabulary. Discarding it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 821/821 [02:14<00:00,  6.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "> DataLoader initialization\n",
      "| > Tokenizer:\n",
      "\t| > add_blank: True\n",
      "\t| > use_eos_bos: False\n",
      "\t| > use_phonemes: True\n",
      "\t| > phonemizer:\n",
      "\t\t| > phoneme language: en-us\n",
      "\t\t| > phoneme backend: gruut\n",
      "\t| > 1 not found characters:\n",
      "\t| > ͡\n",
      "| > Number of instances : 821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m > TRAINING (2025-05-22 13:30:22) \u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | > Preprocessing samples\n",
      " | > Max text length: 173\n",
      " | > Min text length: 6\n",
      " | > Avg text length: 55.60535931790499\n",
      " | \n",
      " | > Max audio length: 358870.0\n",
      " | > Min audio length: 25558.0\n",
      " | > Avg audio length: 110688.91352009744\n",
      " | > Num. instances discarded samples: 0\n",
      " | > Batch group size: 32.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda\\envs\\vc\\lib\\site-packages\\torch\\functional.py:704: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
      "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\SpectralOps.cpp:878.)\n",
      "  return _VF.stft(  # type: ignore[attr-defined]\n",
      "d:\\miniconda\\envs\\vc\\lib\\site-packages\\TTS\\tts\\models\\vits.py:1273: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=False):  # use float32 for the criterion\n",
      "d:\\miniconda\\envs\\vc\\lib\\site-packages\\TTS\\tts\\models\\vits.py:1284: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=False):\n",
      "d:\\miniconda\\envs\\vc\\lib\\site-packages\\TTS\\tts\\models\\vits.py:1311: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=False):  # use float32 for the criterion\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-22 13:30:35 -- STEP: 0/52 -- GLOBAL_STEP: 0\u001b[0m\n",
      "     | > loss_disc: 5.949109077453613  (5.949109077453613)\n",
      "     | > loss_disc_real_0: 0.9778444170951843  (0.9778444170951843)\n",
      "     | > loss_disc_real_1: 0.9998602271080017  (0.9998602271080017)\n",
      "     | > loss_disc_real_2: 1.0166256427764893  (1.0166256427764893)\n",
      "     | > loss_disc_real_3: 0.9675069451332092  (0.9675069451332092)\n",
      "     | > loss_disc_real_4: 0.9778657555580139  (0.9778657555580139)\n",
      "     | > loss_disc_real_5: 1.0087900161743164  (1.0087900161743164)\n",
      "     | > loss_0: 5.949109077453613  (5.949109077453613)\n",
      "     | > grad_norm_0: tensor(6.6446, device='cuda:0')  (tensor(6.6446, device='cuda:0'))\n",
      "     | > loss_gen: 4.4822587966918945  (4.4822587966918945)\n",
      "     | > loss_kl: 129.752685546875  (129.752685546875)\n",
      "     | > loss_feat: 0.15130959451198578  (0.15130959451198578)\n",
      "     | > loss_mel: 116.32991790771484  (116.32991790771484)\n",
      "     | > loss_duration: 2.5839200019836426  (2.5839200019836426)\n",
      "     | > loss_1: 253.3000946044922  (253.3000946044922)\n",
      "     | > grad_norm_1: tensor(1125.0145, device='cuda:0')  (tensor(1125.0145, device='cuda:0'))\n",
      "     | > current_lr_0: 0.0002 \n",
      "     | > current_lr_1: 0.0002 \n",
      "     | > step_time: 13.0018  (13.001770973205566)\n",
      "     | > loader_time: 0.4611  (0.4610707759857178)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-22 13:35:11 -- STEP: 25/52 -- GLOBAL_STEP: 25\u001b[0m\n",
      "     | > loss_disc: 2.9713058471679688  (3.196694278717041)\n",
      "     | > loss_disc_real_0: 0.25250816345214844  (0.27189222574234007)\n",
      "     | > loss_disc_real_1: 0.24045903980731964  (0.2883753187954426)\n",
      "     | > loss_disc_real_2: 0.2597362995147705  (0.2885087206959725)\n",
      "     | > loss_disc_real_3: 0.2601810693740845  (0.29356030285358425)\n",
      "     | > loss_disc_real_4: 0.24575577676296234  (0.30694989025592795)\n",
      "     | > loss_disc_real_5: 0.2669377624988556  (0.3237376570701599)\n",
      "     | > loss_0: 2.9713058471679688  (3.196694278717041)\n",
      "     | > grad_norm_0: tensor(0.9667, device='cuda:0')  (tensor(2.3816, device='cuda:0'))\n",
      "     | > loss_gen: 1.5582267045974731  (1.678588399887085)\n",
      "     | > loss_kl: 5.689139366149902  (16.067373638153075)\n",
      "     | > loss_feat: 0.37870854139328003  (0.2263596138358116)\n",
      "     | > loss_mel: 60.44224548339844  (79.95707244873047)\n",
      "     | > loss_duration: 2.104926586151123  (2.3170831108093255)\n",
      "     | > loss_1: 70.17324829101562  (100.24647705078125)\n",
      "     | > grad_norm_1: tensor(207.2257, device='cuda:0')  (tensor(252.9420, device='cuda:0'))\n",
      "     | > current_lr_0: 0.0002 \n",
      "     | > current_lr_1: 0.0002 \n",
      "     | > step_time: 11.8099  (10.827953510284424)\n",
      "     | > loader_time: 0.1213  (0.10459792137145996)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-22 13:41:22 -- STEP: 50/52 -- GLOBAL_STEP: 50\u001b[0m\n",
      "     | > loss_disc: 2.835050582885742  (3.050494771003723)\n",
      "     | > loss_disc_real_0: 0.23139435052871704  (0.2600653028488159)\n",
      "     | > loss_disc_real_1: 0.2546456754207611  (0.2695480454713106)\n",
      "     | > loss_disc_real_2: 0.26890310645103455  (0.27012840285897255)\n",
      "     | > loss_disc_real_3: 0.20660091936588287  (0.27183527559041976)\n",
      "     | > loss_disc_real_4: 0.1924436390399933  (0.275896257162094)\n",
      "     | > loss_disc_real_5: 0.17577871680259705  (0.28356461703777314)\n",
      "     | > loss_0: 2.835050582885742  (3.050494771003723)\n",
      "     | > grad_norm_0: tensor(6.5630, device='cuda:0')  (tensor(3.7678, device='cuda:0'))\n",
      "     | > loss_gen: 1.717240810394287  (1.6596591401100158)\n",
      "     | > loss_kl: 2.6027743816375732  (9.699557623863223)\n",
      "     | > loss_feat: 0.623162567615509  (0.41578555092215536)\n",
      "     | > loss_mel: 54.16709899902344  (67.83667671203611)\n",
      "     | > loss_duration: 2.1106791496276855  (2.1996500635147087)\n",
      "     | > loss_1: 61.22095489501953  (81.81132873535157)\n",
      "     | > grad_norm_1: tensor(179.3957, device='cuda:0')  (tensor(257.4463, device='cuda:0'))\n",
      "     | > current_lr_0: 0.0002 \n",
      "     | > current_lr_1: 0.0002 \n",
      "     | > step_time: 29.4519  (12.71971839427948)\n",
      "     | > loader_time: 0.5522  (0.1632835865020751)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | > Synthesizing test sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda\\envs\\vc\\lib\\site-packages\\TTS\\tts\\models\\vits.py:1455: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3687.)\n",
      "  test_figures[\"{}-alignment\".format(idx)] = plot_alignment(alignment.T, output_fig=False)\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time: 0.16208044220419485 \u001b[0m(+0)\n",
      "     | > avg_step_time: 12.575639149721932 \u001b[0m(+0)\n",
      "     | > avg_loss_disc: 3.0456783117032518 \u001b[0m(+0)\n",
      "     | > avg_loss_disc_real_0: 0.25952082960044637 \u001b[0m(+0)\n",
      "     | > avg_loss_disc_real_1: 0.2691716201895592 \u001b[0m(+0)\n",
      "     | > avg_loss_disc_real_2: 0.2705564628921303 \u001b[0m(+0)\n",
      "     | > avg_loss_disc_real_3: 0.27019652081470863 \u001b[0m(+0)\n",
      "     | > avg_loss_disc_real_4: 0.2752129537802115 \u001b[0m(+0)\n",
      "     | > avg_loss_disc_real_5: 0.2832507356709125 \u001b[0m(+0)\n",
      "     | > avg_loss_0: 3.0456783117032518 \u001b[0m(+0)\n",
      "     | > avg_grad_norm_0: tensor(3.7854, device='cuda:0') \u001b[0m(+0)\n",
      "     | > avg_loss_gen: 1.6618466096765854 \u001b[0m(+0)\n",
      "     | > avg_loss_kl: 9.548747137481094 \u001b[0m(+0)\n",
      "     | > avg_loss_feat: 0.42215345171736735 \u001b[0m(+0)\n",
      "     | > avg_loss_mel: 67.6496855791877 \u001b[0m(+0)\n",
      "     | > avg_loss_duration: 2.1977055166281896 \u001b[0m(+0)\n",
      "     | > avg_loss_1: 81.48013799330768 \u001b[0m(+0)\n",
      "     | > avg_grad_norm_1: tensor(256.4887, device='cuda:0') \u001b[0m(+0)\n",
      "\n",
      " > BEST MODEL : artifacts/model_training/output\\vits_ljspeech-May-22-2025_01+27PM-f4cb597\\best_model_52.pth\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 1/200\u001b[0m\n",
      " --> artifacts/model_training/output\\vits_ljspeech-May-22-2025_01+27PM-f4cb597\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-22 13:41:38) \u001b[0m\n",
      " > Keyboard interrupt detected.\n",
      " > Saving model before exiting...\n",
      "\n",
      " > CHECKPOINT : artifacts/model_training/output\\vits_ljspeech-May-22-2025_01+27PM-f4cb597\\checkpoint_69.pth\n",
      " ! Run is kept in artifacts/model_training/output\\vits_ljspeech-May-22-2025_01+27PM-f4cb597\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config=ConfigurationManager()\n",
    "    model_training_config=config.get_model_training_config()\n",
    "    model_training=ModelConfig(config=model_training_config)\n",
    "    model_training.get_audio_config()\n",
    "    model_training.get_dataset_config()\n",
    "    model_training.get_vits_config()\n",
    "    model_training.get_fit()    \n",
    "    \n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0f7cdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
